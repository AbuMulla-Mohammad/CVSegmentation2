{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbuMulla-Mohammad/CVSegmentation2/blob/main/DocumentSegmentation2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXgi95ehhPmi",
        "outputId": "e5993ddb-ebc3-419c-c183-d0f1f3b1f1b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-5.8.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting boto3<2.0.0,>=1.34.0 (from cohere)\n",
            "  Downloading boto3-1.35.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting httpx>=0.21.2 (from cohere)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting httpx-sse==0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.8.2)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.20.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.19.1)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.0.20240712-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (4.12.2)\n",
            "Collecting botocore<1.36.0,>=1.35.0 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading botocore-1.35.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx>=0.21.2->cohere)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.21.2->cohere)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.0.7)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<1,>=0.15->cohere) (0.23.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.0->boto3<2.0.0,>=1.34.0->cohere) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.66.5)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.0->boto3<2.0.0,>=1.34.0->cohere) (1.16.0)\n",
            "Downloading cohere-5.8.1-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.8/207.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading boto3-1.35.0-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading types_requests-2.32.0.20240712-py3-none-any.whl (15 kB)\n",
            "Downloading botocore-1.35.0-py3-none-any.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: types-requests, parameterized, jmespath, httpx-sse, h11, fastavro, httpcore, botocore, s3transfer, httpx, boto3, cohere\n",
            "Successfully installed boto3-1.35.0 botocore-1.35.0 cohere-5.8.1 fastavro-1.9.5 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 httpx-sse-0.4.0 jmespath-1.0.1 parameterized-0.9.0 s3transfer-0.10.2 types-requests-2.32.0.20240712\n",
            "Collecting instructor\n",
            "  Downloading instructor-1.3.7-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from instructor) (3.10.2)\n",
            "Requirement already satisfied: docstring-parser<0.17,>=0.16 in /usr/local/lib/python3.10/dist-packages (from instructor) (0.16)\n",
            "Collecting jiter<0.5.0,>=0.4.1 (from instructor)\n",
            "  Downloading jiter-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting openai<2.0.0,>=1.1.0 (from instructor)\n",
            "  Downloading openai-1.41.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from instructor) (2.8.2)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from instructor) (2.20.1)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /usr/local/lib/python3.10/dist-packages (from instructor) (13.7.1)\n",
            "Collecting tenacity<9.0.0,>=8.4.1 (from instructor)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from instructor) (0.12.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (4.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.1.0->instructor) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.1.0->instructor) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.1.0->instructor) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.1.0->instructor) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.1.0->instructor) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.1.0->instructor) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.8.0->instructor) (0.7.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->instructor) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->instructor) (2.16.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.9.0->instructor) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.9.0->instructor) (1.5.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.1.0->instructor) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.1.0->instructor) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (0.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor) (0.1.2)\n",
            "Downloading instructor-1.3.7-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (327 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.6/327.6 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.41.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.4/362.4 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: tenacity, jiter, openai, instructor\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed instructor-1.3.7 jiter-0.4.2 openai-1.41.0 tenacity-8.5.0\n",
            "Collecting docx\n",
            "  Downloading docx-0.2.4.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from docx) (4.9.4)\n",
            "Requirement already satisfied: Pillow>=2.0 in /usr/local/lib/python3.10/dist-packages (from docx) (9.4.0)\n",
            "Building wheels for collected packages: docx\n",
            "  Building wheel for docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53892 sha256=82b6124511ebdb63e34b041c73beeade67a7dbe0d981109d6ce1e3965f5e5e37\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/f5/1d/e09ba2c1907a43a4146d1189ae4733ca1a3bfe27ee39507767\n",
            "Successfully built docx\n",
            "Installing collected packages: docx\n",
            "Successfully installed docx-0.2.4\n",
            "Requirement already satisfied: instructor in /usr/local/lib/python3.10/dist-packages (1.3.7)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from instructor) (3.10.2)\n",
            "Requirement already satisfied: docstring-parser<0.17,>=0.16 in /usr/local/lib/python3.10/dist-packages (from instructor) (0.16)\n",
            "Requirement already satisfied: jiter<0.5.0,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from instructor) (0.4.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from instructor) (1.41.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from instructor) (2.8.2)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from instructor) (2.20.1)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /usr/local/lib/python3.10/dist-packages (from instructor) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.4.1 in /usr/local/lib/python3.10/dist-packages (from instructor) (8.5.0)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from instructor) (0.12.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (4.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.1.0->instructor) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.1.0->instructor) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.1.0->instructor) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.1.0->instructor) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.1.0->instructor) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.1.0->instructor) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.8.0->instructor) (0.7.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->instructor) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->instructor) (2.16.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.9.0->instructor) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.9.0->instructor) (1.5.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.1.0->instructor) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.1.0->instructor) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (0.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor) (0.1.2)\n",
            "Collecting generativeai\n",
            "  Downloading generativeai-0.0.1-py3-none-any.whl.metadata (479 bytes)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from generativeai) (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->generativeai) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->generativeai) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->generativeai) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->generativeai) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->generativeai) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->generativeai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->generativeai) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->generativeai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->generativeai) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->generativeai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->generativeai) (2024.7.4)\n",
            "Downloading generativeai-0.0.1-py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: generativeai\n",
            "Successfully installed generativeai-0.0.1\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 sentence-transformers-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install cohere\n",
        "!pip install instructor\n",
        "!pip install docx\n",
        "!pip install instructor\n",
        "!pip install generativeai\n",
        "!pip install python-docx\n",
        "!pip install sentence-transformers transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oI7Ku3dhmQM",
        "outputId": "c16b6d80-05d2-41f0-a5b3-f07221f8fdcf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
            "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
            "* 'smart_union' has been removed\n",
            "  warnings.warn(message, UserWarning)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import cohere\n",
        "from docx import Document\n",
        "import os\n",
        "from transformers import pipeline\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from docx.oxml.table import CT_Tbl\n",
        "from docx.oxml.text.paragraph import CT_P\n",
        "import instructor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuHgERgZhqZI"
      },
      "outputs": [],
      "source": [
        "class Section(BaseModel):\n",
        "    title: str = Field(description=\"Main topic of this section of the document\")\n",
        "    start_index: int = Field(description=\"Line number where the section begins\")\n",
        "    end_index: int = Field(description=\"Line number where the section ends\")\n",
        "    category: str = Field(description=\"The Category of this section\")\n",
        "    content: str = Field(description=\"The actual content of this section\")\n",
        "\n",
        "    def matches(self, job_application_section: str) -> bool:\n",
        "        return job_application_section.lower() in self.content.lower()\n",
        "class StructuredDocument(BaseModel):\n",
        "    \"\"\"Obtains meaningful sections, each centered around a single concept/topic.\"\"\"\n",
        "    sections: List[Section] = Field(description=\"A list of sections of the document\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwHJb58flePR"
      },
      "outputs": [],
      "source": [
        "def calculate_similarity_TF_IDF(segments, job_sections):\n",
        "    cv_texts = [segment['content'] for segment in segments]\n",
        "    job_texts = [job['content'] for job in job_sections]\n",
        "    all_texts = cv_texts + job_texts\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
        "    cv_vectors = tfidf_matrix[:len(cv_texts)]\n",
        "    job_vectors = tfidf_matrix[len(cv_texts):]\n",
        "\n",
        "    similarity_matrix = cosine_similarity(cv_vectors, job_vectors)\n",
        "\n",
        "    similarities = []\n",
        "    for i, segment in enumerate(segments):\n",
        "        print(f\"Similarity for CV Section '{segment['title']}':\")\n",
        "        for j, job_section in enumerate(job_sections):\n",
        "            similarity = similarity_matrix[i][j] * 100  # Convert to percentage\n",
        "            similarities.append({\n",
        "                \"User_Skills\": i,\n",
        "                \"Job_Requirements\": j,\n",
        "                \"Match_Score\": similarity\n",
        "            })\n",
        "            print(f\"  - With Job Section '{job_section['title']}': {similarity:.2f}%\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    return similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeClmP5hKsOv"
      },
      "outputs": [],
      "source": [
        "def compute_similarity_between_sections(cv_sections, job_sections):\n",
        "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "    cv_texts = [section['content'] for section in cv_sections]\n",
        "    job_texts = [section['content'] for section in job_sections]\n",
        "    cv_embeddings = model.encode(cv_texts, convert_to_tensor=True)\n",
        "    job_embeddings = model.encode(job_texts, convert_to_tensor=True)\n",
        "\n",
        "    similarity_matrix = []\n",
        "    for i, cv_embedding in enumerate(cv_embeddings):\n",
        "        similarity_row = []\n",
        "        for j, job_embedding in enumerate(job_embeddings):\n",
        "            similarity = util.pytorch_cos_sim(cv_embedding, job_embedding).item() * 100\n",
        "            similarity_row.append(similarity)\n",
        "        similarity_matrix.append(similarity_row)\n",
        "\n",
        "    similarities = []\n",
        "    for i, section in enumerate(cv_sections):\n",
        "        print(f\"Similarity for CV Section '{section['title']}':\")\n",
        "        for j, job_section in enumerate(job_sections):\n",
        "            similarity = similarity_matrix[i][j]\n",
        "            similarities.append({\n",
        "                \"User_Skills\": i,\n",
        "                \"Job_Requirements\": j,\n",
        "                \"Match_Score\": similarity\n",
        "            })\n",
        "            print(f\"  - With Job Section '{job_section['title']}': {similarity:.2f}%\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    return similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UplTUA3WWWtJ"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL_RLg-BWAen"
      },
      "outputs": [],
      "source": [
        "def compute_similarity_between_sections_using_distilbert(cv_sections, job_sections):\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "    model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    def encode_texts(texts, tokenizer, model):\n",
        "        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "        return embeddings\n",
        "\n",
        "    cv_texts = [section['content'] for section in cv_sections]\n",
        "    job_texts = [section['content'] for section in job_sections]\n",
        "\n",
        "    cv_embeddings = encode_texts(cv_texts, tokenizer, model)\n",
        "    job_embeddings = encode_texts(job_texts, tokenizer, model)\n",
        "\n",
        "    similarity_matrix = cosine_similarity(cv_embeddings.numpy(), job_embeddings.numpy())\n",
        "\n",
        "    similarities = []\n",
        "    for i, section in enumerate(cv_sections):\n",
        "        print(f\"Similarity for CV Section '{section['title']}':\")\n",
        "        for j, job_section in enumerate(job_sections):\n",
        "            similarity = similarity_matrix[i][j] * 100  # Convert to percentage\n",
        "            similarities.append({\n",
        "                \"User_Skills\": i,\n",
        "                \"Job_Requirements\": j,\n",
        "                \"Match_Score\": similarity\n",
        "            })\n",
        "            print(f\"  - With Job Section '{job_section['title']}': {similarity:.2f}%\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    return similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w6GiogIaG8F"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_squared_error(predicted_similarities, target_similarities):\n",
        "    predicted_scores = np.array([sim['similarity_score'] for sim in predicted_similarities])\n",
        "    target_scores = np.array(target_similarities)\n",
        "    squared_errors = np.square(predicted_scores - target_scores)\n",
        "    return squared_errors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhp5G4OMaJGb"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def calculate_mse(predicted_similarities, true_similarities):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    print(\"predicted_similarities\",predicted_similarities,\"true_similarities\",true_similarities)\n",
        "    for key, true_value in true_similarities.items():\n",
        "        if key in predicted_similarities:\n",
        "            y_true.append(true_value)\n",
        "            y_pred.append(predicted_similarities[key])\n",
        "\n",
        "    return mean_squared_error(y_true, y_pred)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YlATpTJNh-GU",
        "outputId": "ce088630-4715-4831-c990-60b3177a182c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Structured Document:\n",
            "\n",
            "Segments:\n",
            "Title: Personal Information\n",
            "Start: 0\n",
            "End: 5\n",
            "Category: Personal Information\n",
            "Content:\n",
            "Sanad Ahmad      Mobile: 0594 55 80 90                                                                                    \n",
            "                                           Email: sanad.ahmed2000@gmail.com\n",
            "\n",
            "\t\t\tAddress: Jaba' – Jenin – Palestine\n",
            "\t\t\tJobs:  IT Project Manager | AI Trainer\n",
            "----------------------------------------\n",
            "Title: Academic Qualifications\n",
            "Start: 8\n",
            "End: 15\n",
            "Category: Education\n",
            "Content:\n",
            "** Academic Qualifications\n",
            "Degree\tTrack\tInstitution\tYear\tGPA\n",
            "Tawjihi\tScientific\tSecondary School\t2007\t3.7\n",
            "Bachelor\tIT: Computer Science\tArab American University\t2011\t3.76\n",
            "Master\tComputer Science: Machine Learning\tArab American University\t2021\t4.00\n",
            "Ph.D. Candidate\tArtificial Intelligence:\n",
            "Natural Language Processing - NLP\tBirzeit University\t2024\t3.7\n",
            "----------------------------------------\n",
            "Title: Publications\n",
            "Start: 16\n",
            "End: 23\n",
            "Category: Publications\n",
            "Content:\n",
            "\n",
            "** Publications\n",
            "NLU-STR at SemEval-2024 Task 1: Generative-based Augmentation and Encoder-based Scoring for Semantic Textual Relatedness\n",
            "Context-Gloss Augmentation for Improving Arabic Target Sense Verification\n",
            "Salma: Arabic sense-annotated corpus and wsd benchmarks\n",
            "Classification and Prediction of Low-Density Lipoprotein Cholesterol LDL-C …\n",
            "Detecting Network Traffic-based Attacks Using ANNs\n",
            "----------------------------------------\n",
            "Title: Work Experience\n",
            "Start: 24\n",
            "End: 35\n",
            "Category: Work Experience\n",
            "Content:\n",
            "\n",
            "** Work Experience\n",
            "Dimensions InfoTech – Palestine (http://dimensions-infotech.com) \n",
            "Eclaims and Integration Specialist Jan 2018 – Now\n",
            "Dimensions Healthcare – Dubai (http://www.dimensions-healthcare.com/) \n",
            "Software Development (4 years) & Team Lead (2 years): Jan 2012 - Jan 2018\n",
            "Arab American University – Palestine (https://www.aaup.edu/) \n",
            "Teaching Assistant: Jan 2011 - Jan 2012\n",
            "Robotics & AI – Palestine\n",
            "Trainer (Workshops & Training Programs): Jan 2021 - Now\n",
            "\n",
            "----------------------------------------\n",
            "Title: Key Experiences\n",
            "Start: 36\n",
            "End: 46\n",
            "Category: Skills\n",
            "Content:\n",
            "** Key Experiences\n",
            "\n",
            "Business \tTechnical\n",
            "Markets: Palestine, Dubai, and Saudi Arabia\tTechnology: ASP.Net (Forms), WCF, EF6\n",
            "IT Project Management & Team Lead\tRobotics, AI, & NLP: Python + Models\n",
            "System Analysis\tWeb App\\Service Development\n",
            "Technical Consultation\tCode Management: GitLab, VSTS, SVN\n",
            "Clients Relationships\tHelp Desk & Support\n",
            "Business Development\tResearch & Teaching Assistance\n",
            "\n",
            "----------------------------------------\n",
            "Title: Languages\n",
            "Start: 47\n",
            "End: 47\n",
            "Category: Languages\n",
            "Content:\n",
            "\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-f23ba53af932>\u001b[0m in \u001b[0;36m<cell line: 90>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mjob_sections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_job_sections_from_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model 1 _________________________________________________________________________________________________\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mcompute_similarity_between_sections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_sections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_sections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-f23ba53af932>\u001b[0m in \u001b[0;36mget_job_sections_from_user\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_job_sections_from_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mjob_sections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mnum_sections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter the number of job sections: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_sections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Enter title for job section {i + 1}: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "api_key = \"QRgahiOvBH2HBDFdd9ye1veAkzpliD4t2VD4LxiJ\"\n",
        "if not api_key:\n",
        "    raise ValueError(\"API key for Cohere is not set in the .env file.\")\n",
        "\n",
        "# Initialize the Cohere client with the API key\n",
        "client = cohere.Client(api_key)\n",
        "\n",
        "# Apply the patch to the Cohere client\n",
        "client = instructor.from_cohere(client)\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a skilled resume analyst tasked with organizing a CV.\n",
        "Read the document below and extract a StructuredDocument object from it where each section of the CV is clearly defined, categorized, and labeled with an appropriate category, the category must be one of these(Personal Information, Objective or Summary, Education, Work Experience, Skills, Languages, Certifications, Projects, Achievements, Publications, Conferences and Seminars, Volunteer Experience, Professional Memberships, Hobbies and Interests, References).\n",
        "Each section should represent a distinct part of the CV, such as Education, Experience, Skills, etc.\n",
        "Your task is to identify the start, end, and the category of each section using the line numbers provided in square brackets (e.g., [1], [2], [3], etc.), and assign a category label to each section based on its content.\n",
        "Ensure to analyze and extract information from tables as well, as they may contain crucial details about Education, Experience, Skills, or other sections.\n",
        "Note: that some sections maybe the title in the same line with its content, i want you to take care with it .\n",
        "\"\"\"\n",
        "\n",
        "# Document Processing Functions\n",
        "def doc_with_lines(document: str):\n",
        "    document_lines = document.split(\"\\n\")\n",
        "    document_with_line_numbers = \"\"\n",
        "    line2text = {}\n",
        "    for i, line in enumerate(document_lines):\n",
        "        document_with_line_numbers += f\"[{i}] {line}\\n\"\n",
        "        line2text[i] = line\n",
        "    return document_with_line_numbers, line2text\n",
        "\n",
        "def get_structured_document(document_with_line_numbers: str) -> StructuredDocument:\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"command-r-plus\",\n",
        "        response_model=StructuredDocument,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": document_with_line_numbers},\n",
        "        ],\n",
        "    )\n",
        "    return response\n",
        "\n",
        "def extract_text_from_docx(docx_path: str) -> str:\n",
        "    doc = Document(docx_path)\n",
        "    text = []\n",
        "    nsmap = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
        "    for element in doc.element.body:\n",
        "        if isinstance(element, CT_Tbl):\n",
        "            for row in element.findall('.//w:tr', namespaces=nsmap):\n",
        "                row_text = []\n",
        "                for cell in row.findall('.//w:tc', namespaces=nsmap):\n",
        "                    cell_text = []\n",
        "                    for paragraph in cell.findall('.//w:p', namespaces=nsmap):\n",
        "                        cell_text.append(paragraph.text or '')\n",
        "                    row_text.append(\"\\n\".join(cell_text))\n",
        "                text.append(\"\\t\".join(row_text))\n",
        "        elif isinstance(element, CT_P):\n",
        "            text.append(element.text or '')\n",
        "    return \"\\n\".join(text)\n",
        "\n",
        "def get_sections_text(structured_doc: StructuredDocument, line2text: dict) -> list:\n",
        "    segments = []\n",
        "    for s in structured_doc.sections:\n",
        "        contents = [line2text.get(line_id, '') for line_id in range(s.start_index, s.end_index)]\n",
        "        segments.append({\n",
        "            \"title\": s.title,\n",
        "            \"content\": \"\\n\".join(contents),\n",
        "            \"start\": s.start_index,\n",
        "            \"end\": s.end_index,\n",
        "            \"category\": s.category\n",
        "        })\n",
        "    return segments\n",
        "\n",
        "def get_job_sections_from_user():\n",
        "    job_sections = []\n",
        "    num_sections = int(input(\"Enter the number of job sections: \"))\n",
        "    for i in range(num_sections):\n",
        "        title = input(f\"Enter title for job section {i + 1}: \")\n",
        "        text = input(f\"Enter content for job section {i + 1}: \")\n",
        "        job_sections.append({\n",
        "            'idx': str(i),\n",
        "            'title': title,\n",
        "            'content': text\n",
        "        })\n",
        "    return job_sections\n",
        "\n",
        "\n",
        "\n",
        "# Main Script\n",
        "docx_path = '/files/SanadAhmad-CV.docx'\n",
        "\n",
        "try:\n",
        "    document = extract_text_from_docx(docx_path)\n",
        "    document_with_line_numbers, line2text = doc_with_lines(document)\n",
        "    structured_doc = get_structured_document(document_with_line_numbers)\n",
        "    cv_sections = get_sections_text(structured_doc, line2text)\n",
        "\n",
        "    print(\"Structured Document:\")\n",
        "    print(\"\\nSegments:\")\n",
        "    for section in cv_sections:\n",
        "        print(f\"Title: {section['title']}\")\n",
        "        print(f\"Start: {section['start']}\")\n",
        "        print(f\"End: {section['end']}\")\n",
        "        print(f\"Category: {section['category']}\")\n",
        "        print(\"Content:\")\n",
        "        print(section['content'])\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    job_sections = get_job_sections_from_user()\n",
        "    print(\"Model 1 _________________________________________________________________________________________________\")\n",
        "    compute_similarity_between_sections(cv_sections, job_sections)\n",
        "    print(\"Model 2 _________________________________________________________________________________________________\")\n",
        "    calculate_similarity_TF_IDF(cv_sections, job_sections)\n",
        "    print(\"Model 3 _________________________________________________________________________________________________\")\n",
        "    compute_similarity_between_sections_using_distilbert(cv_sections, job_sections)\n",
        "    # Calculate MSE for each model\n",
        "\n",
        "    import pandas as pd\n",
        "\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv('/files/Job Datsset.csv')\n",
        "    print(df)\n",
        "    # Extract ground truth similarity scores\n",
        "    true_similarities = df['Match_Score'].to_dict()\n",
        "    print(df)\n",
        "    # Example for TF-IDF\n",
        "    predicted_similarities_tfidf = {\n",
        "        ( item['Job_Requirements']): item['Match_Score']\n",
        "        for item in calculate_similarity_TF_IDF(cv_sections, job_sections)\n",
        "    }\n",
        "\n",
        "    # Example for Sentence Transformers\n",
        "    predicted_similarities_sbert = {\n",
        "        ( item['Job_Requirements']): item['Match_Score']\n",
        "        for item in compute_similarity_between_sections(cv_sections, job_sections)\n",
        "    }\n",
        "\n",
        "    # Example for DistilBERT\n",
        "    predicted_similarities_distilbert = {\n",
        "        ( item['Job_Requirements']): item['Match_Score']\n",
        "        for item in compute_similarity_between_sections_using_distilbert(cv_sections, job_sections)\n",
        "    }\n",
        "    mse_distilbert = calculate_mse(predicted_similarities_distilbert, true_similarities)\n",
        "    mse_tfidf = calculate_mse(predicted_similarities_tfidf, true_similarities)\n",
        "    mse_sbert = calculate_mse(predicted_similarities_sbert, true_similarities)\n",
        "\n",
        "    print(f\"MSE for TF-IDF: {mse_tfidf:.4f}\")\n",
        "    print(f\"MSE for Sentence Transformers: {mse_sbert:.4f}\")\n",
        "    print(f\"MSE for DistilBERT: {mse_distilbert:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOA5FO7MsQP6MivIg05XIvG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}